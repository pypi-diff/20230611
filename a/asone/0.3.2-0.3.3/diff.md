# Comparing `tmp/asone-0.3.2-py3-none-any.whl.zip` & `tmp/asone-0.3.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,26 +1,26 @@
-Zip file size: 477468 bytes, number of entries: 243
+Zip file size: 477411 bytes, number of entries: 243
 -rw-rw-r--  2.0 unx     2850 b- defN 23-May-23 13:52 asone/__init__.py
--rw-rw-r--  2.0 unx     7408 b- defN 23-Apr-12 09:28 asone/asone.py
--rw-rw-r--  2.0 unx     2715 b- defN 23-Jun-03 07:02 asone/demo_detector.py
+-rw-rw-r--  2.0 unx    10674 b- defN 23-Jun-11 02:29 asone/asone.py
+-rw-rw-r--  2.0 unx     2634 b- defN 23-Jun-11 02:52 asone/demo_detector.py
 -rw-rw-r--  2.0 unx     2149 b- defN 23-Apr-20 08:09 asone/demo_ocr.py
--rw-rw-r--  2.0 unx     2449 b- defN 23-Jun-03 07:07 asone/demo_pose_estimator.py
--rw-rw-r--  2.0 unx     1713 b- defN 23-Apr-20 08:09 asone/demo_tracker.py
+-rw-rw-r--  2.0 unx     2451 b- defN 23-Jun-05 08:30 asone/demo_pose_estimator.py
+-rw-rw-r--  2.0 unx     1722 b- defN 23-Jun-06 09:49 asone/demo_tracker.py
 -rw-rw-r--  2.0 unx     2942 b- defN 23-May-23 13:34 asone/pose_estimator.py
 -rw-rw-r--  2.0 unx      573 b- defN 23-May-23 13:52 asone/detectors/__init__.py
--rw-rw-r--  2.0 unx     5251 b- defN 23-May-23 13:52 asone/detectors/detector.py
+-rw-rw-r--  2.0 unx     5450 b- defN 23-Jun-08 15:14 asone/detectors/detector.py
 -rw-rw-r--  2.0 unx       66 b- defN 23-Apr-10 06:15 asone/detectors/easyocr_detector/__init__.py
 -rw-rw-r--  2.0 unx     2094 b- defN 23-Apr-20 08:09 asone/detectors/easyocr_detector/text_detector.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Apr-10 06:15 asone/detectors/utils/__init__.py
 -rw-rw-r--  2.0 unx      533 b- defN 23-Apr-10 06:15 asone/detectors/utils/cfg_path.py
 -rw-rw-r--  2.0 unx     1545 b- defN 23-Apr-20 08:09 asone/detectors/utils/coreml_utils.py
 -rw-rw-r--  2.0 unx     1478 b- defN 23-Apr-10 06:15 asone/detectors/utils/exp_name.py
 -rw-rw-r--  2.0 unx     9122 b- defN 23-May-23 13:52 asone/detectors/utils/weights_path.py
 -rw-rw-r--  2.0 unx       66 b- defN 23-May-23 13:52 asone/detectors/yolonas/__init__.py
--rw-rw-r--  2.0 unx     3769 b- defN 23-May-23 13:52 asone/detectors/yolonas/yolonas.py
+-rw-rw-r--  2.0 unx     3957 b- defN 23-Jun-11 01:48 asone/detectors/yolonas/yolonas.py
 -rw-rw-r--  2.0 unx       69 b- defN 23-Apr-10 06:15 asone/detectors/yolor/__init__.py
 -rw-rw-r--  2.0 unx     5315 b- defN 23-Apr-20 08:09 asone/detectors/yolor/yolor_detector.py
 -rw-rw-r--  2.0 unx    14241 b- defN 23-Apr-10 06:15 asone/detectors/yolor/cfg/yolor_csp.cfg
 -rw-rw-r--  2.0 unx    16338 b- defN 23-Apr-10 06:15 asone/detectors/yolor/cfg/yolor_csp_x.cfg
 -rw-rw-r--  2.0 unx    18330 b- defN 23-Apr-10 06:15 asone/detectors/yolor/cfg/yolor_p6.cfg
 -rw-rw-r--  2.0 unx        1 b- defN 23-Apr-10 06:15 asone/detectors/yolor/models/__init__.py
 -rw-rw-r--  2.0 unx    38971 b- defN 23-Apr-10 06:15 asone/detectors/yolor/models/common.py
@@ -232,14 +232,14 @@
 -rw-rw-r--  2.0 unx      558 b- defN 23-Apr-10 06:15 asone/utils/counting.py
 -rw-rw-r--  2.0 unx      358 b- defN 23-Apr-10 06:15 asone/utils/default_cfg.py
 -rw-rw-r--  2.0 unx     5728 b- defN 23-May-23 13:52 asone/utils/download.py
 -rw-rw-r--  2.0 unx    11844 b- defN 23-May-23 13:34 asone/utils/draw.py
 -rw-rw-r--  2.0 unx      864 b- defN 23-Apr-10 06:15 asone/utils/ponits_conversion.py
 -rw-rw-r--  2.0 unx     1017 b- defN 23-May-23 13:34 asone/utils/pose_estimators_weights.py
 -rw-rw-r--  2.0 unx      796 b- defN 23-Apr-10 06:15 asone/utils/temp_loader.py
--rw-rw-r--  2.0 unx    35148 b- defN 23-Jun-03 07:17 asone-0.3.2.dist-info/LICENCE
--rw-rw-r--  2.0 unx    14376 b- defN 23-Jun-03 07:17 asone-0.3.2.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jun-03 07:17 asone-0.3.2.dist-info/WHEEL
--rw-rw-r--  2.0 unx       72 b- defN 23-Jun-03 07:17 asone-0.3.2.dist-info/dependency_links.txt
--rw-rw-r--  2.0 unx        6 b- defN 23-Jun-03 07:17 asone-0.3.2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    24134 b- defN 23-Jun-03 07:17 asone-0.3.2.dist-info/RECORD
-243 files, 1691196 bytes uncompressed, 438068 bytes compressed:  74.1%
+-rw-rw-r--  2.0 unx    35148 b- defN 23-Jun-11 02:54 asone-0.3.3.dist-info/LICENCE
+-rw-rw-r--  2.0 unx    14376 b- defN 23-Jun-11 02:54 asone-0.3.3.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-11 02:54 asone-0.3.3.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       72 b- defN 23-Jun-11 02:54 asone-0.3.3.dist-info/dependency_links.txt
+-rw-rw-r--  2.0 unx        6 b- defN 23-Jun-11 02:54 asone-0.3.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    24135 b- defN 23-Jun-11 02:54 asone-0.3.3.dist-info/RECORD
+243 files, 1694780 bytes uncompressed, 438011 bytes compressed:  74.2%
```

## zipnote {}

```diff
@@ -705,26 +705,26 @@
 
 Filename: asone/utils/pose_estimators_weights.py
 Comment: 
 
 Filename: asone/utils/temp_loader.py
 Comment: 
 
-Filename: asone-0.3.2.dist-info/LICENCE
+Filename: asone-0.3.3.dist-info/LICENCE
 Comment: 
 
-Filename: asone-0.3.2.dist-info/METADATA
+Filename: asone-0.3.3.dist-info/METADATA
 Comment: 
 
-Filename: asone-0.3.2.dist-info/WHEEL
+Filename: asone-0.3.3.dist-info/WHEEL
 Comment: 
 
-Filename: asone-0.3.2.dist-info/dependency_links.txt
+Filename: asone-0.3.3.dist-info/dependency_links.txt
 Comment: 
 
-Filename: asone-0.3.2.dist-info/top_level.txt
+Filename: asone-0.3.3.dist-info/top_level.txt
 Comment: 
 
-Filename: asone-0.3.2.dist-info/RECORD
+Filename: asone-0.3.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## asone/asone.py

```diff
@@ -14,32 +14,33 @@
 class ASOne:
     def __init__(self,
                  detector: int = 0,
                  tracker: int = -1,
                  weights: str = None,
                  use_cuda: bool = True,
                  recognizer: int = None,
-                 languages: list = ['en']
+                 languages: list = ['en'],
+                 num_classes=80
                  ) -> None:
 
         self.use_cuda = use_cuda
 
         # get detector object
-        self.detector = self.get_detector(detector, weights, recognizer)
+        self.detector = self.get_detector(detector, weights, recognizer, num_classes)
         self.recognizer = self.get_recognizer(recognizer, languages=languages)
     
         if tracker == -1:
             self.tracker = None
             return
             
         self.tracker = self.get_tracker(tracker)
 
-    def get_detector(self, detector: int, weights: str, recognizer):
+    def get_detector(self, detector: int, weights: str, recognizer, num_classes):
         detector = Detector(detector, weights=weights,
-                            use_cuda=self.use_cuda, recognizer=recognizer).get_detector()
+                            use_cuda=self.use_cuda, recognizer=recognizer, num_classes=num_classes).get_detector()
         return detector
 
     def get_recognizer(self, recognizer: int, languages):
         if recognizer == None:
             return None
         recognizer = TextRecognizer(recognizer,
                             use_cuda=self.use_cuda, languages=languages).get_recognizer()
@@ -81,14 +82,107 @@
         kwargs['filename'] = output_filename
         config = self._update_args(kwargs)
 
         for (bbox_details, frame_details) in self._start_tracking(video_path, config):
             # yeild bbox_details, frame_details to main script
             yield bbox_details, frame_details
 
+    def detect_video(self,
+                    video_path,
+                    **kwargs
+                    ):            
+        output_filename = os.path.basename(video_path)
+        kwargs['filename'] = output_filename
+        config = self._update_args(kwargs)
+        
+        # os.makedirs(output_path, exist_ok=True)
+
+        fps = config.pop('fps')
+        output_dir = config.pop('output_dir')
+        filename = config.pop('filename')
+        save_result = config.pop('save_result')
+        display = config.pop('display')
+        draw_trails = config.pop('draw_trails')
+        class_names = config.pop('class_names')
+
+        cap = cv2.VideoCapture(video_path)
+        width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
+        height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
+        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
+
+        if fps is None:
+            fps = cap.get(cv2.CAP_PROP_FPS)
+
+        if save_result:
+            os.makedirs(output_dir, exist_ok=True)
+            save_path = os.path.join(output_dir, filename)
+            logger.info(f"video save path is {save_path}")
+
+            video_writer = cv2.VideoWriter(
+                save_path,
+                cv2.VideoWriter_fourcc(*"mp4v"),
+                fps,
+                (int(width), int(height)),
+            )
+
+        frame_id = 1
+        tic = time.time()
+
+        prevTime = 0
+        frame_no = 0
+        while True:
+            start_time = time.time()
+
+            ret, img = cap.read()
+            if not ret:
+                break
+            frame = img.copy()
+            
+            dets, img_info = self.detector.detect(img, conf_thres=0.25, iou_thres=0.45)
+            currTime = time.time()
+            fps = 1 / (currTime - prevTime)
+            prevTime = currTime
+
+            if dets is not None: 
+                bbox_xyxy = dets[:, :4]
+                scores = dets[:, 4]
+                class_ids = dets[:, 5]
+                img = utils.draw_boxes(img, bbox_xyxy, class_ids=class_ids, class_names=class_names)
+
+            cv2.line(img, (20, 25), (127, 25), [85, 45, 255], 30)
+            cv2.putText(img, f'FPS: {int(fps)}', (11, 35), 0, 1, [
+                        225, 255, 255], thickness=2, lineType=cv2.LINE_AA)
+
+
+            elapsed_time = time.time() - start_time
+
+            logger.info(
+                'frame {}/{} ({:.2f} ms)'.format(frame_no, int(frame_count),
+                                                 elapsed_time * 1000))
+            frame_no+=1
+            if display:
+                cv2.imshow('Window', img)
+
+            if save_result:
+                video_writer.write(img)
+
+            if cv2.waitKey(25) & 0xFF == ord('q'):
+                break
+            
+            yield (bbox_xyxy, scores, class_ids), (im0 if display else frame, frame_no-1, fps)
+
+        tac = time.time()
+        print(f'Total Time Taken: {tac - tic:.2f}')
+        # kwargs['filename'] = output_filename
+        # config = self._update_args(kwargs)
+        
+        # for (bbox_details, frame_details) in self._start_tracking(video_path, config):
+        #     # yeild bbox_details, frame_details to main script
+        #     yield bbox_details, frame_details
+    
     def detect(self, source, **kwargs)->np.ndarray:
         """ Function to perform detection on an img
 
         Args:
             source (_type_): if str read the image. if nd.array pass it directly to detect
 
         Returns:
```

## asone/demo_detector.py

```diff
@@ -1,85 +1,65 @@
+import sys
+import argparse
 import asone
 from asone import ASOne
-from .utils import draw_boxes
-import cv2
-import argparse
-import time
-import os
+import torch
+
 
 def main(args):
     filter_classes = args.filter_classes
-    video_path = args.video
-
-    os.makedirs(args.output_path, exist_ok=True)
 
     if filter_classes:
-        filter_classes = filter_classes.split(',')
-
+        filter_classes = ['person']
+    # Check if cuda available
+    if args.use_cuda and torch.cuda.is_available():
+        args.use_cuda = True
+    else:
+        args.use_cuda = False
+     
+    if sys.platform.startswith('darwin'):
+        detector = asone.YOLOV7_MLMODEL 
+    else:
+        detector = asone.YOLOV7_PYTORCH
     
-    detector = ASOne(asone.YOLOV7_PYTORCH, weights=args.weights, use_cuda=args.use_cuda)
-
-    cap = cv2.VideoCapture(video_path)
-    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
-    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
-    FPS = cap.get(cv2.CAP_PROP_FPS)
-
-    if args.save:
-        video_writer = cv2.VideoWriter(
-            os.path.basename(video_path),
-            cv2.VideoWriter_fourcc(*"mp4v"),
-            FPS,
-            (int(width), int(height)),
+    detect = ASOne(
+        detector=detector,
+        weights=args.weights,
+        use_cuda=args.use_cuda
         )
+    # Get tracking function
+    track = detect.detect_video(args.video_path,
+                                output_dir=args.output_dir,
+                                conf_thres=args.conf_thres,
+                                iou_thres=args.iou_thres,
+                                display=args.display,
+                                filter_classes=filter_classes,
+                                class_names=None) # class_names=['License Plate'] for custom weights
     
-    frame_no = 1
-    tic = time.time()
-
-    prevTime = 0
-
-    while True:
-        start_time = time.time()
-
-        ret, img = cap.read()
-        if not ret:
-            break
-        frame = img.copy()
+    # Loop over track_fn to retrieve outputs of each frame 
+    for bbox_details, frame_details in track:
+        bbox_xyxy, scores, class_ids = bbox_details
+        frame, frame_num, fps = frame_details
+        print(frame_num)
         
-        dets, img_info = detector.detect(img, conf_thres=0.25, iou_thres=0.45)
-        currTime = time.time()
-        fps = 1 / (currTime - prevTime)
-        prevTime = currTime
 
-        if dets is not None: 
-            bbox_xyxy = dets[:, :4]
-            scores = dets[:, 4]
-            class_ids = dets[:, 5]
-            img = draw_boxes(img, bbox_xyxy, class_ids=class_ids)
-
-        cv2.line(img, (20, 25), (127, 25), [85, 45, 255], 30)
-        cv2.putText(img, f'FPS: {int(fps)}', (11, 35), 0, 1, [
-                    225, 255, 255], thickness=2, lineType=cv2.LINE_AA)
-
-
-        frame_no+=1
-        if args.display:
-            cv2.imshow('Window', img)
-
-        if args.save:
-            video_writer.write(img)
-
-        if cv2.waitKey(25) & 0xFF == ord('q'):
-            break
-
-if __name__=='__main__':
-    
+if __name__ == '__main__':
     parser = argparse.ArgumentParser()
-    parser.add_argument("video", help="Path of video")
-    parser.add_argument('--cpu', default=True, action='store_false', dest='use_cuda', help='If provided the model will run on cpu otherwise it will run on gpu')
-    parser.add_argument('--filter_classes', default=None, help='Class names seperated by comma (,). e.g. person,car ')
+
+    parser.add_argument('video_path', help='Path to input video')
+    parser.add_argument('--cpu', default=True, action='store_false', dest='use_cuda',
+                        help='run on cpu if not provided the program will run on gpu.')
+    parser.add_argument('--no_save', default=True, action='store_false',
+                        dest='save_result', help='whether or not save results')
+    parser.add_argument('--no_display', default=True, action='store_false',
+                        dest='display', help='whether or not display results on screen')
+    parser.add_argument('--output_dir', default='data/results',  help='Path to output directory')
+    parser.add_argument('--draw_trails', action='store_true', default=False,
+                        help='if provided object motion trails will be drawn.')
+    parser.add_argument('--filter_classes', default=None, help='Filter class name')
     parser.add_argument('-w', '--weights', default=None, help='Path of trained weights')
-    parser.add_argument('-o', '--output_path', default='data/results', help='path of output file')
-    parser.add_argument('--no_display', action='store_false', default=True, dest='display', help='if provided video will not be displayed')
-    parser.add_argument('--no_save', action='store_false', default=True, dest='save', help='if provided video will not be saved')
+    parser.add_argument('-ct', '--conf_thres', default=0.25, type=float, help='confidence score threshold')
+    parser.add_argument('-it', '--iou_thres', default=0.45, type=float, help='iou score threshold')
 
     args = parser.parse_args()
+
     main(args)
```

## asone/demo_pose_estimator.py

```diff
@@ -7,15 +7,15 @@
 import os
 
 
 def main(args):
     
     video_path = args.video
     os.makedirs(args.output_path, exist_ok=True)
-    estimator = PoseEstimator(asone.YOLOV8L_POSE, weights=args.weights, use_cuda=args.use_cuda)
+    estimator = PoseEstimator(asone.YOLOV7_W6_POSE, weights=args.weights, use_cuda=args.use_cuda)
 
     cap = cv2.VideoCapture(video_path)
     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
     FPS = cap.get(cv2.CAP_PROP_FPS)
 
     if args.save:
```

## asone/demo_tracker.py

```diff
@@ -14,15 +14,15 @@
     os.makedirs(args.output_path, exist_ok=True)
 
     if filter_classes:
         filter_classes = filter_classes.split(',')
 
 
     detect = ASOne(tracker=asone.BYTETRACK, detector=asone.YOLOV7_PYTORCH, 
-                   use_cuda=True)
+                   use_cuda=args.use_cuda)
 
     track = detect.track_video(video_path, output_dir=args.output_path, 
                                save_result=args.save, display=args.display,
                                filter_classes=filter_classes)
  
     for bbox_details, frame_details in track:
         bbox_xyxy, ids, scores, class_ids = bbox_details
```

## asone/detectors/detector.py

```diff
@@ -15,18 +15,19 @@
 
 
 class Detector:
     def __init__(self,
                  model_flag: int,
                  weights: str = None,
                  use_cuda: bool = True,
-                 recognizer:int = None):
+                 recognizer:int = None,
+                 num_classes=80):
         
-        self.model = self._select_detector(model_flag, weights, use_cuda, recognizer)
-    def _select_detector(self, model_flag, weights, cuda, recognizer):
+        self.model = self._select_detector(model_flag, weights, use_cuda, recognizer, num_classes)
+    def _select_detector(self, model_flag, weights, cuda, recognizer, num_classes):
         # Get required weight using model_flag
         mlmodel = False
         if weights and weights.split('.')[-1] == 'onnx':
             onnx = True
             weight = weights
         elif weights and weights.split('.')[-1] == 'mlmodel':
             onnx = False
@@ -97,17 +98,20 @@
             # Get exp file and corresponding model for coreml only
             _detector = YOLOv8Detector(weights=weight,
                                        use_onnx=onnx,
                                        mlmodel=mlmodel,
                                        use_cuda=cuda)
         elif model_flag in range(160, 163):
             # Get exp file and corresponding model for coreml only
-            _detector = YOLOnasDetector(weights=weight,
-                                       use_onnx=onnx,
-                                       use_cuda=cuda)
+            _detector = YOLOnasDetector(
+                                    model_flag,
+                                    weights=weight,
+                                    use_onnx=onnx,
+                                    use_cuda=cuda,
+                                    num_classes=num_classes)
             
         return _detector
 
     def get_detector(self):
         return self.model
 
     def detect(self,
```

## asone/detectors/yolonas/yolonas.py

```diff
@@ -4,79 +4,93 @@
 import warnings
 import torch
 import onnxruntime
 from asone import utils
 import super_gradients
 import numpy as np
 from super_gradients.training.processing import DetectionCenterPadding, StandardizeImage, NormalizeImage, ImagePermute, ComposeProcessing, DetectionLongestMaxSizeRescale
+from super_gradients.training import models
+from super_gradients.common.object_names import Models
 
 
-class_names = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17",
-               "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32", "33", "34",
-               "35", "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49", "50", "51",
-               "52", "53", "54", "55", "56", "57", "58", "59", "60", "61", "62", "63","64", "65", "66", "67", "68",
-               "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79"]
+class_names = [""]
 
 
 class YOLOnasDetector:
     def __init__(self,
+                 model_flag,
                  weights=None,
                  cfg=None,
                  use_onnx=True,
                  use_cuda=True,
+                #  checkpoint_num_classes=80,
+                 num_classes=80
                  ):
         
+        
+        self.model_flag = model_flag
+        # self.checkpoint_num_classes = checkpoint_num_classes
         if not os.path.exists(weights):
             utils.download_weights(weights)
-            
+        
+        self.num_classes = num_classes
         self.device = 'cuda' if use_cuda and torch.cuda.is_available() else 'cpu'
         self.use_onnx = use_onnx
 
         # Load Model
         self.model = self.load_model(weights=weights)
 
     def load_model(self, weights):
-        model_name = os.path.basename(weights)
-        name, file_extension = os.path.splitext(model_name)
-        
-        model = super_gradients.training.models.get(name, checkpoint_path=weights, checkpoint_num_classes=80, num_classes=80).to(self.device)
         
+            # model = super_gradients.training.models.get(name, 
+            #             checkpoint_path=weights, 
+            #             checkpoint_num_classes=self.checkpoint_num_classes,
+            #             num_classes=self.num_classes).to(self.device)
+    
+        if self.model_flag == 160: 
+            model = models.get(Models.YOLO_NAS_S,
+                    checkpoint_path=weights,
+                    num_classes=self.num_classes).to(self.device)
+        elif self.model_flag == 161:
+            model = models.get(Models.YOLO_NAS_M,
+                    checkpoint_path=weights,
+                    num_classes=self.num_classes).to(self.device)
+        elif self.model_flag == 162:
+            model = models.get(Models.YOLO_NAS_L,
+                    checkpoint_path=weights,
+                    num_classes=self.num_classes).to(self.device)
         return model
+    
     def detect(self, image: list,
                input_shape: tuple = (640, 640),
                conf_thres: float = 0.25,
                iou_thres: float = 0.45,
                max_det: int = 1000,
                filter_classes: bool = None,
                agnostic_nms: bool = True,
                with_p6: bool = False,
                return_image=False) -> list:
 
-        
-        self.model.set_dataset_processing_params( class_names=class_names,
-        image_processor=ComposeProcessing(
-                [
-                    DetectionLongestMaxSizeRescale(output_shape=(636, 636)),
-                    DetectionCenterPadding(output_shape=(640, 640), pad_value=114),
-                    StandardizeImage(max_value=255.0),
-                    ImagePermute(permutation=(2, 0, 1)),
-                ]
-            ),
-        iou=iou_thres,conf=conf_thres,
-        )
+        if self.num_classes==80:
+            self.model.set_dataset_processing_params(class_names=class_names,
+            image_processor=ComposeProcessing(
+                    [
+                        DetectionLongestMaxSizeRescale(output_shape=(636, 636)),
+                        DetectionCenterPadding(output_shape=(640, 640), pad_value=114),
+                        StandardizeImage(max_value=255.0),
+                        ImagePermute(permutation=(2, 0, 1)),
+                    ]
+                ),
+            iou=iou_thres,conf=conf_thres,
+            )
         original_image = image
         # Inference
         if self.use_onnx:
             pass
-            # Input names of ONNX model on which it is exported
-            # input_name = self.model.get_inputs()[0].name
-            # # Run onnx model
-            # pred = self.model.run([self.model.get_outputs()[0].name], {
-            #                       input_name: processed_image})[0]
-            # Run Pytorch model
+            
         else:
 
             detections = self.model.predict(image)
             image_info = {
                 'width': original_image.shape[1],
                 'height': original_image.shape[0],
             }
```

## Comparing `asone-0.3.2.dist-info/LICENCE` & `asone-0.3.3.dist-info/LICENCE`

 * *Files identical despite different names*

## Comparing `asone-0.3.2.dist-info/METADATA` & `asone-0.3.3.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: asone
-Version: 0.3.2
+Version: 0.3.3
 Summary: UNKNOWN
 Home-page: https://github.com/axcelerateai/asone
 Author: AxcelerateAI
 Author-email: umair.imran@axcelerate.ai
 License: BSD 2-clause
 Keywords: asone bytetrack deepsort norfair yolo yolox yolor yolov5 yolov7 installation inferencing
 Platform: UNKNOWN
```

## Comparing `asone-0.3.2.dist-info/RECORD` & `asone-0.3.3.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 asone/__init__.py,sha256=8Gnz7vdGKMgSRVFMmbwKWY_3alFhqyJ-om7F1fq8Z3E,2850
-asone/asone.py,sha256=UazrS9iJ8w4vRBI5wVjJPbpZFdSpVmrf59FaG2Ujdpw,7408
-asone/demo_detector.py,sha256=Va6ahBMHn8R5GA7hrXPvCJ-hcQe5laC52P4hFMNzLXE,2715
+asone/asone.py,sha256=w33igfr9JjHR-uMlJ6ttVroHbh_Y36G_yR2p0pvmQ8c,10674
+asone/demo_detector.py,sha256=_QtQhnOEnLaKaoCkZaHFQQcqLuZtiZpVFNdFnBjcj0o,2634
 asone/demo_ocr.py,sha256=W-pRUC_z52Wb-9_C8f12N25npc3jIy-l0sttn_ylE3M,2149
-asone/demo_pose_estimator.py,sha256=rSj9wp3b1ESrySRCl7cHxzR-L0Irjq0XybU149wIvJM,2449
-asone/demo_tracker.py,sha256=e9QUI-DYL1LJv7SN6utCYmupCcRiSO5rt2Fv6DAQlqU,1713
+asone/demo_pose_estimator.py,sha256=ryQUAcu7LZsVUcJImtEFAMK9zAB0iwMeShljPUNfNd8,2451
+asone/demo_tracker.py,sha256=pWdFogJL3W2yR_UmN4997fGUYe0xiYkfHRHhzx_G3vA,1722
 asone/pose_estimator.py,sha256=v7bKzunE57ppM7B7cRjAaWZ-VI5XxzLvkUa0-kwPa-w,2942
 asone/detectors/__init__.py,sha256=k8C_M140PId6CXC9Pkupjj4hrISCwNyz_7xVyW9DAig,573
-asone/detectors/detector.py,sha256=HELHR9HZi3FD2qNZGk88xwv0msD0VjRpG8qVFVToFjc,5251
+asone/detectors/detector.py,sha256=TytcwUosYhEekYtWYdPe7735d_qqc3__mj_48PznqHg,5450
 asone/detectors/easyocr_detector/__init__.py,sha256=s6CVaMCy-Y4-k7gxZ6jCg28mGp_YxrvweOICgMwbLzc,66
 asone/detectors/easyocr_detector/text_detector.py,sha256=l_Ao5QIO39v2FoXKROrt0wezaToPE9IWGj-HwihfCOk,2094
 asone/detectors/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 asone/detectors/utils/cfg_path.py,sha256=meeJjd_Ici6Khmf7YjuTErq4mrJAE1qDqhtXPDFiSn0,533
 asone/detectors/utils/coreml_utils.py,sha256=G4qkahiEnLxHPT_nxI0SAZ7Qy34r3zf036ZSNhsF_Gc,1545
 asone/detectors/utils/exp_name.py,sha256=znQ0oMgfyFSEWI6YOgjekVsGdWOeOI-Uom9BLdL9YNY,1478
 asone/detectors/utils/weights_path.py,sha256=kkET2tvvEtlKCBtP7vJiF1i5_ZGWh13-mFu73aLs-7s,9122
 asone/detectors/yolonas/__init__.py,sha256=KZhNS-C5my65fEwx8kgnnRU0p6UN8svdT04tc2EuyPU,66
-asone/detectors/yolonas/yolonas.py,sha256=vhMgqmUtJXU-9XofK_bEnyURR5rnTe5hNbevpEclHlw,3769
+asone/detectors/yolonas/yolonas.py,sha256=PQzosYKeLuoitNTxNuRt2KZIjhPl02oiNT52R9nh86Q,3957
 asone/detectors/yolor/__init__.py,sha256=rK6ZEZXSC091X3i_towa6w7-DC0ABVOMF3tsUZdqftA,69
 asone/detectors/yolor/yolor_detector.py,sha256=GPfszy9rdsv1F_8kdUScrQdrGcSRzLT6y41lHKiQ_3M,5315
 asone/detectors/yolor/cfg/yolor_csp.cfg,sha256=03194cNE7d-d3freWbD6Jfn1qIl6clliprVW8nHYj8E,14241
 asone/detectors/yolor/cfg/yolor_csp_x.cfg,sha256=jokGophucWBWc7YB5mnBffkUWmojxu-a4Z2aKa54FO8,16338
 asone/detectors/yolor/cfg/yolor_p6.cfg,sha256=4sEvW_-v-cZU9Azpx-oG9eU1RstD4Ni1Q2ZuElCo-6o,18330
 asone/detectors/yolor/models/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
 asone/detectors/yolor/models/common.py,sha256=e23HDO0VBXkT__m8Smgq-iY1XCPj7eR16bfDIBvPJjc,38971
@@ -231,13 +231,13 @@
 asone/utils/counting.py,sha256=PThbapxvLFn6ftbEbV9kxHvWmBuXLglR33-athJa8EQ,558
 asone/utils/default_cfg.py,sha256=UzCAEVMGSLbeqYgmr3y5-F3rgXCVqdB8Hm7nPsOlMhU,358
 asone/utils/download.py,sha256=vz4xD9pW6PzToYeVGMD3_GT77IEaVn1R0wiiHSTcZQ4,5728
 asone/utils/draw.py,sha256=z_7MMSEQy14uOv-d-_mOhEKd3ZLTVm3_uqby-VlgOxU,11844
 asone/utils/ponits_conversion.py,sha256=rlPuPNplz2WzsIQScS4xRvwxkDKr18WI2kr9HI3W-iY,864
 asone/utils/pose_estimators_weights.py,sha256=d66imkevci3qQYD9u-XScWrJ2LnM_bNVuIGZzBePFcQ,1017
 asone/utils/temp_loader.py,sha256=K8NRezJkFkB_XQbcgkl1-h1pTWRV528ntqMwXuABaCs,796
-asone-0.3.2.dist-info/LICENCE,sha256=ixuiBLtpoK3iv89l7ylKkg9rs2GzF9ukPH7ynZYzK5s,35148
-asone-0.3.2.dist-info/METADATA,sha256=Y0FLypCiIkKfqUW-Cm9bZy-U3uSzTcUj9cV-frqJfkM,14376
-asone-0.3.2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-asone-0.3.2.dist-info/dependency_links.txt,sha256=Yt-SL99DmwySbgxGwJLEZqCU9bLw-mgok8v_E4vqBlE,72
-asone-0.3.2.dist-info/top_level.txt,sha256=n-xJvkjLnGLv_U9fB26iIIWEXbTcOkw2hvrAPeWaPUE,6
-asone-0.3.2.dist-info/RECORD,,
+asone-0.3.3.dist-info/LICENCE,sha256=ixuiBLtpoK3iv89l7ylKkg9rs2GzF9ukPH7ynZYzK5s,35148
+asone-0.3.3.dist-info/METADATA,sha256=xONknUFiX-7cV4OQOhipP3jqgeRZjHiBlqH1Sx-5Hvk,14376
+asone-0.3.3.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+asone-0.3.3.dist-info/dependency_links.txt,sha256=Yt-SL99DmwySbgxGwJLEZqCU9bLw-mgok8v_E4vqBlE,72
+asone-0.3.3.dist-info/top_level.txt,sha256=n-xJvkjLnGLv_U9fB26iIIWEXbTcOkw2hvrAPeWaPUE,6
+asone-0.3.3.dist-info/RECORD,,
```

